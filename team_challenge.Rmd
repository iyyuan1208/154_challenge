---
title: "MATH154 Team Challenge"
author: "NAMES"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

### Loading the packages:

```{r, warning=FALSE, message=FALSE}
library(e1071)
library(ggplot2)
library(plyr)
library(tidyverse)
library(mice)
library(randomForest)
library(class)
library(caret)
```

### EDA

We began our analysis by first loading the training data set and then examine the predicators.

```{r}
data_train <- read.csv('data/cs-training.csv')
colnames(data_train)
```


#### Portion of defaulted

```{r}
mean(data_train$SeriousDlqin2yrs)
```

We then check each feature with **summary()** and see which of these features have null data and how many.

```{r}
data_col <- colnames(data_train)
for(i in 2:12){
  print(data_col[i])
  print(summary(data_train[,i]))
}
```

This suggests that only monthly income and number of dependents have missing data, which we would later either fill in or drop. We then examine each variable to check for the existence of outliers

```{r}
data_train$SeriousDlqin2yrs <- as.factor(data_train$SeriousDlqin2yrs)
```


#### Revolving-Utilization-Of-Unsecured-Lines

For the second variable **Revolving-Utilization-Of-Unsecured-Lines**, which measures the total balance on credit card divided by sum of credit limits (amounts owing divided by total available for borrowing), the max number is $50708$, which is unlikely as we can't borrow beyond the limit by that much. 

Take a look at Observations with **Revolving-Utilization-Of-Unsecured-Lines** $>1$ and $>100$. There are 3338 obs with **Revolving-Utilization-Of-Unsecured-Lines** $>1$ and 223 obs with **Revolving-Utilization-Of-Unsecured-Lines** $>100$.

```{r}
g1 <- subset(data_train,RevolvingUtilizationOfUnsecuredLines>=1)
g2 <- subset(data_train,RevolvingUtilizationOfUnsecuredLines>=100)
summary(g1)
summary(g2)
```

If we remove the outliers using the $1.5$IQR rule and plot the density plot grouped by whether there is a financial stress experienced, we observe an interesting shape. The group that has experienced financial stress is more likely to be concentrated on the higher end of the value of the **RevolvingUtilizationOfUnsecuredLines** variable.

```{r}
Q <- quantile(data_train$RevolvingUtilizationOfUnsecuredLines, probs=c(.25, .75), na.rm = TRUE)
iqr <- IQR(data_train$RevolvingUtilizationOfUnsecuredLines, na.rm = TRUE)
data_revu <- subset(data_train, 
                        data_train$RevolvingUtilizationOfUnsecuredLines > (Q[1] - 1.5*iqr) &
                          data_train$RevolvingUtilizationOfUnsecuredLines < (Q[2]+1.5*iqr))
```

```{r}
ggplot(data = data_revu,
            mapping = aes(x = RevolvingUtilizationOfUnsecuredLines,
                          after_stat(density), colour = SeriousDlqin2yrs)) +
  geom_freqpoly()
```

```{r, message=FALSE, include=FALSE}
ggplot(data_revu, aes(x=RevolvingUtilizationOfUnsecuredLines, color=SeriousDlqin2yrs,
                       fill=SeriousDlqin2yrs)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")
```

```{r}
nrow(data_revu)
summary(data_revu$RevolvingUtilizationOfUnsecuredLines)
```

#### Age

An analysis of the third variable **age** shows that the group who have experienced financial distress in the next two years have an average of age lower than the other group who have not experienced such stress. This may suggest that young people are more likely to experience financial hardships relative to older people. Additionally, the histogram shows that there are far more people who have not experienced any financial distress than the other group.

```{r}
mage <- ddply(data_train, "SeriousDlqin2yrs", summarise, grp.mean=mean(age))
head(mage)
```

```{r, message=FALSE}
ggplot(data_train, aes(x=age, color=SeriousDlqin2yrs,
                       fill=SeriousDlqin2yrs)) +
  geom_vline(data=mage, aes(xintercept=grp.mean, color=SeriousDlqin2yrs),
                                  linetype="dashed", size=1.3) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")
```

```{r}
data_train %>%
  ggplot(aes(x = SeriousDlqin2yrs, y = age)) + 
  geom_boxplot(color = 'black', fill = 'firebrick') +
  labs(x = "SeriousDlqin2yrs", y = "age")

t.test(age ~ SeriousDlqin2yrs, data = data_train, var.equal = TRUE)
```

Since p-value is much smaller than the conventional $0.05$ threshold, we say that we have found statistical significance in comparing the average of age between the two groups. 

#### NumberOfTime30.59DaysPastDueNotWorse, and NumberOfTime60.89DaysPastDueNotWorse and NumberOfTime90DaysPastDueNotWorse

In this variable, the max number is 98, which is not possible since $98\times 30 =2940$ days, which is equivalent to 8 years. However, the variable measures how many times the person has been 30-59 days past dues for the past 2 years, which makes the value 98 impossible. We should remove any value >24.33 as outliers. 

Still, looking at the summary statistics stated below, we found that for the group who have experienced financial stress, their mean and standard deviation are both significantly higher than the group who have not.

```{r}
data_train %>%
  group_by(SeriousDlqin2yrs) %>%
  summarise(
          count = n(),
          mean_ntimes = mean(NumberOfTime30.59DaysPastDueNotWorse),
          sd_ntimes = sd(NumberOfTime30.59DaysPastDueNotWorse),
          min_ntimes = min(NumberOfTime30.59DaysPastDueNotWorse),
          max_ntimes = max(NumberOfTime30.59DaysPastDueNotWorse)
            )
```

```{r}
table(data_train$NumberOfTime30.59DaysPastDueNotWorse)
print('NumberOfTime30.59DaysPastDueNotWorse')
table(data_train$NumberOfTime60.89DaysPastDueNotWorse)
print('NumberOfTime60.89DaysPastDueNotWorse')
table(data_train$NumberOfTimes90DaysLate)
print('NumberOfTimes90DaysLate')
summary(subset(data_train, NumberOfTime30.59DaysPastDueNotWorse>=96))
summary(subset(data_train, NumberOfTime60.89DaysPastDueNotWorse>=96))
summary(subset(data_train, NumberOfTimes90DaysLate>=96))
table(subset(data_train, NumberOfTime30.59DaysPastDueNotWorse>=96)$X == (subset(data_train, NumberOfTime60.89DaysPastDueNotWorse>=96))$X)
table(subset(data_train, NumberOfTime30.59DaysPastDueNotWorse>=96)$X == (subset(data_train, NumberOfTimes90DaysLate>=96)$X))
```


#### DebtRatio

There seem to be some abnormalities in the distribution of the debt ratio, which should be a percentage that is the Monthly debt payments, alimony, living costs divided by monthy gross income. It is possible for this number to be greater than one, but for some observations, the number is already over $1000$, which seem to be quite impossible in real life.

```{r}
data_train %>%
  group_by(SeriousDlqin2yrs) %>%
  summarise(
          count = n(),
          mean = mean(DebtRatio),
          sd = sd(DebtRatio),
          min = min(DebtRatio),
          max = max(DebtRatio)
            )
quantile(data_train$DebtRatio, probs=c(.25, .75), na.rm = TRUE)
```

In the following steps, we first remove the outliers contained in the training data based on the **DebtRatio** variable, and instead of plotting the counts, we choose to plot the density plot and observe how the curves changes as the **DebtRatio** variable increases on the horizontal axis. 

```{r}
Q <- quantile(data_train$DebtRatio, probs=c(.25, .75), na.rm = TRUE)
iqr <- IQR(data_train$DebtRatio, na.rm = TRUE)
data_dbtratio <- subset(data_train, 
                        data_train$DebtRatio > (Q[1] - 1.5*iqr) &
                          data_train$DebtRatio < (Q[2]+1.5*iqr))
range(data_dbtratio$DebtRatio)
nrow(data_dbtratio)
```

We observe that for the group without experiencing any financial stress, it has a distribution that is higher than the other gorup on the lower end of the debt ratio. As the raio increases, we observe that the density curve for the group with experience of financial stress becomes higher.

```{r}
ggplot(data = data_dbtratio,
            mapping = aes(x = DebtRatio, after_stat(density), colour = SeriousDlqin2yrs)) +
  geom_freqpoly()
```


#### MonthlyIncome

The summary statistics show that the group without financial stress has a higher mean/median monthly income than the group with financial stress, but we also observe that some people have a monthly income of `r max(data_train$MonthlyIncome, na.rm=TRUE)`, which is unusually high.


```{r}
data_train %>%
  group_by(SeriousDlqin2yrs) %>%
  summarise(
          count = n(),
          mean = mean(MonthlyIncome, na.rm = TRUE),
          median = median(MonthlyIncome, na.rm = TRUE),
          sd = sd(MonthlyIncome, na.rm = TRUE),
          min = min(MonthlyIncome, na.rm = TRUE),
          max = max(MonthlyIncome, na.rm = TRUE)
            )
```

#### Number of Dependents

```{r}
data_train %>%
  group_by(SeriousDlqin2yrs) %>%
  summarise(
          count = n(),
          mean = mean(NumberOfDependents, na.rm = TRUE),
          median = median(NumberOfDependents, na.rm = TRUE),
          sd = sd(MonthlyIncome, na.rm = TRUE),
          min = min(NumberOfDependents, na.rm = TRUE),
          max = max(NumberOfDependents, na.rm = TRUE)
            )
```

```{r, message=FALSE}
ggplot(data_dbtratio, aes(x=NumberOfDependents, color=SeriousDlqin2yrs,
                       fill=SeriousDlqin2yrs)) +
  geom_histogram(alpha = 0.5) +
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")
```

### Data Cleaning

We thought about dropping the missing values, replacing missing values with medians, and using regressions to replace missing values

```{r}
# remove 1.5 IQR of both Revolving Utilization of Unsecured Lines and Debt Ration.
Q1 <- quantile(data_train$RevolvingUtilizationOfUnsecuredLines, probs=c(.25, .75), na.rm = TRUE)
iqr1 <- IQR(data_train$RevolvingUtilizationOfUnsecuredLines, na.rm = TRUE)
Q2 <- quantile(data_train$DebtRatio, probs=c(.25, .75), na.rm = TRUE)
iqr2 <- IQR(data_train$DebtRatio, na.rm = TRUE)

data_removed <- subset(data_train, 
                        data_train$RevolvingUtilizationOfUnsecuredLines > (Q1[1] - 1.5*iqr1) &
                          data_train$RevolvingUtilizationOfUnsecuredLines < (Q1[2] + 1.5*iqr1))
data_removed <- subset(data_removed, 
                        data_removed$DebtRatio > (Q2[1] - 1.5 * iqr2) &
                         data_removed$DebtRatio < (Q2[2] + 1.5 * iqr2))

#summary(data_removed)
#mean(as.numeric(data_train$SeriousDlqin2yrs) - 1)
#mean(as.numeric(data_removed$SeriousDlqin2yrs) - 1) # the removal leaves with the similar percentage of defaulter

# Remove 96's and 98's in 30-59, 60-89, 90 days past due in the data. 

data_removed <- subset(data_removed,
                       data_removed$NumberOfTimes90DaysLate < 96)
#summary(data_removed)
```

#increse the percentage of defualted by randomly sampling non-defaulted data
```{r}
data_default <- subset(data_removed, data_removed$SeriousDlqin2yrs == 1)
data_nodefault <- subset(data_removed, data_removed$SeriousDlqin2yrs == 0)
data_nodefault_sample <- data_nodefault[sample(1:nrow(data_nodefault), nrow(data_default)*7/3),]
data_removed_sample <- merge(data_nodefault_sample, data_default, all =TRUE)
summary(data_removed_sample)
```
#### Imputation using random forest on sampled data
```{r}
#random forest imputation on data_removed training set 
set.seed(47)
md.pattern(data_removed_sample[,c(7,12)])
data_removed_sample_rfimpute <- mice(data_removed_sample[,-c(1,2)], meth = 'rf', m = 1)
```

#### Imputation using random forest
```{r}
#random forest imputation on data_removed training set 
set.seed(47)
md.pattern(data_removed[,c(7,12)])
data_removed_rfimpute <- mice(data_removed[,-c(1,2)], meth = 'rf', m = 1)
```

```{r}
#random forest imputation on data_test testing set
data_test <- read.csv('data/cs-test.csv')
md.pattern(data_test[,-2])
set.seed(47)
data_test_rfimpute <- mice(data_test[,-c(1,2)],meth = 'rf', m = 1)
```
```{r}
completed_data_removed_rfimpute <- complete(data_removed_rfimpute, 1)
completed_data_test_rfimpute <- complete(data_test_rfimpute, 1)
```

####Replacing NA with Median
```{r}
data_removed_median <- data_removed
median1 <- as.numeric(summary(data_removed_median$MonthlyIncome)[3])
median2 <- as.numeric(summary(data_removed_median$NumberOfDependents)[3])

data_removed_median$MonthlyIncome[is.na(data_removed_median$MonthlyIncome)] <- median1

data_removed_median$NumberOfDependents[is.na(data_removed_median$NumberOfDependents)] <- median2
```

#### Drop NA data in the training set without removing any outlier
```{r}
data_train_drop <- data_train[complete.cases(data_train$MonthlyIncome), ]
data_train_drop <- data_train_drop[complete.cases(data_train_drop$NumberOfDependents), ]
```

#### KNN


#### svm



#### Random Forest

